"""
Script 2: æ‰¹é‡è¯„ä¼°å™¨
ä» CSV æ–‡ä»¶ä¸­è¯»å–æ¨¡å‹å“åº”ï¼Œä½¿ç”¨ LLM è¿›è¡Œè¯„ä¼°ï¼Œå¹¶å°† JSON ç»“æœå±•å¹³ä¸ºå•ç‹¬çš„åˆ—ã€‚
"""

import os
import sys
import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, Optional
import json
import re
import time

# å¯¼å…¥é…ç½®
import config

# å¯¼å…¥å·¥å…·ç±»
from utils import LLMClient

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def extract_evaluation(response_json: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»è¯„ä¼° JSON ä¸­æå–å¹¶è®¡ç®—è¯„åˆ†
    æ”¯æŒä¸¤ç§ JSON æ ¼å¼ï¼š
    1. æ‰å¹³æ ¼å¼: {"scores": {"factuality_score": 9, "completeness_score": 9}}
    2. åµŒå¥—æ ¼å¼: {"scores": {"factuality_safety": {"score": 9, "weight": 0.35}}}
    
    Args:
        response_json: æ¨¡å‹è¿”å›çš„ JSON å¯¹è±¡
    
    Returns:
        åŒ…å«ä¼˜å…ˆçº§ã€è¯„åˆ†ã€å†³ç­–ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    priority = response_json.get("determined_priority", "P3")
    scores = response_json.get("scores", {})
    
    # æ£€æµ‹ JSON æ ¼å¼ï¼šæ‰å¹³æ ¼å¼è¿˜æ˜¯åµŒå¥—æ ¼å¼
    is_nested_format = False
    if scores:
        # æ£€æŸ¥ç¬¬ä¸€ä¸ªå€¼æ˜¯å¦æ˜¯å­—å…¸ï¼ˆåµŒå¥—æ ¼å¼ï¼‰
        first_value = next(iter(scores.values()), None)
        if isinstance(first_value, dict) and ("score" in first_value or "weight" in first_value):
            is_nested_format = True
    
    # æ ¹æ®æ ¼å¼æå–åˆ†æ•°
    if is_nested_format:
        # åµŒå¥—æ ¼å¼ï¼šä»åµŒå¥—å¯¹è±¡ä¸­æå– score
        factuality_score = 0
        completeness_score = 0
        adherence_score = 0
        attractiveness_score = 0
        
        # å°è¯•ä»åµŒå¥—ç»“æ„ä¸­æå–åˆ†æ•°
        # æ”¯æŒå¸¸è§çš„é”®åå˜ä½“
        for key, value in scores.items():
            if isinstance(value, dict):
                score_value = value.get("score", 0)
                key_lower = key.lower()
                
                # åŒ¹é…å„ç§å¯èƒ½çš„é”®å
                if "factuality" in key_lower or "safety" in key_lower:
                    factuality_score = score_value
                elif "completeness" in key_lower or "coverage" in key_lower:
                    completeness_score = score_value
                elif "adherence" in key_lower or "instruction" in key_lower or "compliance" in key_lower:
                    adherence_score = score_value
                elif "attractiveness" in key_lower or "quality" in key_lower or "appeal" in key_lower:
                    attractiveness_score = score_value
    else:
        # æ‰å¹³æ ¼å¼ï¼šç›´æ¥è·å–åˆ†æ•°
        factuality_score = scores.get('factuality_score', 0)
        completeness_score = scores.get('completeness_score', 0)
        adherence_score = scores.get('adherence_score', 0)
        attractiveness_score = scores.get('attractiveness_score', 0)
        
        # å¦‚æœç›´æ¥é”®ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„é”®å
        if factuality_score == 0:
            factuality_score = scores.get('factuality', scores.get('safety_score', 0))
        if completeness_score == 0:
            completeness_score = scores.get('completeness', scores.get('coverage_score', 0))
        if adherence_score == 0:
            adherence_score = scores.get('adherence', scores.get('instruction_score', 0))
        if attractiveness_score == 0:
            attractiveness_score = scores.get('attractiveness', scores.get('quality_score', 0))
    
    # ç¡®ä¿åˆ†æ•°æ˜¯æ•°å€¼ç±»å‹
    try:
        factuality_score = float(factuality_score) if factuality_score else 0
        completeness_score = float(completeness_score) if completeness_score else 0
        adherence_score = float(adherence_score) if adherence_score else 0
        attractiveness_score = float(attractiveness_score) if attractiveness_score else 0
    except (ValueError, TypeError):
        factuality_score = completeness_score = adherence_score = attractiveness_score = 0
    
    # è‡ªåŠ¨æ£€æµ‹åˆ†æ•°åˆ¶å¼ï¼šå¦‚æœæ‰€æœ‰åˆ†æ•°éƒ½ <= 10ï¼Œè®¤ä¸ºæ˜¯ 0-10 åˆ†åˆ¶ï¼›å¦åˆ™è®¤ä¸ºæ˜¯ 0-100 åˆ†åˆ¶
    max_score = max(factuality_score, completeness_score, adherence_score, attractiveness_score)
    is_0_10_scale = max_score <= 10 and max_score > 0
    
    # å¦‚æœæ¨¡å‹å·²ç»æä¾›äº† weighted_total_scoreï¼Œä¼˜å…ˆä½¿ç”¨ï¼ˆä½†éœ€è¦éªŒè¯èŒƒå›´ï¼‰
    model_weighted_score = response_json.get("weighted_total_score")
    if model_weighted_score is not None:
        try:
            model_weighted_score = float(model_weighted_score)
            # å¦‚æœæ¨¡å‹è¿”å›çš„æ˜¯ 0-10 åˆ†åˆ¶ï¼Œè½¬æ¢ä¸º 0-100 åˆ†åˆ¶
            if 0 <= model_weighted_score <= 10:
                model_weighted_score = model_weighted_score * 10
            # å¦‚æœå·²ç»æ˜¯ 0-100 åˆ†åˆ¶ï¼Œç›´æ¥ä½¿ç”¨
            if 0 <= model_weighted_score <= 100:
                weighted_score = model_weighted_score
            else:
                # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œé‡æ–°è®¡ç®—
                if is_0_10_scale:
                    # 0-10 åˆ†åˆ¶ï¼šä½¿ç”¨åŸæƒé‡ï¼ˆ3, 2, 2.5, 2.5ï¼‰
                    weighted_score = (
                        factuality_score * 3 +
                        completeness_score * 2 +
                        adherence_score * 2.5 +
                        attractiveness_score * 2.5
                    )
                else:
                    # 0-100 åˆ†åˆ¶ï¼šä½¿ç”¨æ–°æƒé‡ï¼ˆ0.3, 0.2, 0.25, 0.25ï¼‰
                    weighted_score = (
                        factuality_score * 0.3 +
                        completeness_score * 0.2 +
                        adherence_score * 0.25 +
                        attractiveness_score * 0.25
                    )
        except (ValueError, TypeError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œé‡æ–°è®¡ç®—
            if is_0_10_scale:
                weighted_score = (
                    factuality_score * 3 +
                    completeness_score * 2 +
                    adherence_score * 2.5 +
                    attractiveness_score * 2.5
                )
            else:
                weighted_score = (
                    factuality_score * 0.3 +
                    completeness_score * 0.2 +
                    adherence_score * 0.25 +
                    attractiveness_score * 0.25
                )
    else:
        # è®¡ç®—åŠ æƒæ€»åˆ† (ä»£ç äºŒæ¬¡æ ¡éªŒï¼Œé˜²æ­¢æ¨¡å‹ç®—é”™)
        if is_0_10_scale:
            # 0-10 åˆ†åˆ¶ï¼šä½¿ç”¨åŸæƒé‡ï¼ˆ3, 2, 2.5, 2.5ï¼‰
            weighted_score = (
                factuality_score * 3 +
                completeness_score * 2 +
                adherence_score * 2.5 +
                attractiveness_score * 2.5
            )
        else:
            # 0-100 åˆ†åˆ¶ï¼šä½¿ç”¨æ–°æƒé‡ï¼ˆ0.3, 0.2, 0.25, 0.25ï¼‰
            weighted_score = (
                factuality_score * 0.3 +
                completeness_score * 0.2 +
                adherence_score * 0.25 +
                attractiveness_score * 0.25
            )
    
    # é€»è¾‘åˆ¤å®š
    # 1. å¹»è§‰ç†”æ–­ï¼ˆæ ¹æ®åˆ†æ•°åˆ¶å¼è°ƒæ•´é˜ˆå€¼ï¼‰
    factuality_threshold = 5 if is_0_10_scale else 50
    if factuality_score < factuality_threshold:
        decision = "REJECT"  # ç›´æ¥ä¸¢å¼ƒ
        reason = "Hallucination Detected"
    # 2. è´¨é‡é—¨æ§› (75 åˆ†æ‰å‘å¸ƒï¼Œ0-100 åˆ†åˆ¶)
    elif weighted_score >= 75:
        decision = "PUBLISH"
        reason = "High Quality Score"
    else:
        decision = "REVIEW"  # äººå·¥å¤æ ¸
        reason = "Low Quality Score"
    
    return {
        "priority": priority,
        "factuality_score": factuality_score,
        "completeness_score": completeness_score,
        "adherence_score": adherence_score,
        "attractiveness_score": attractiveness_score,
        "weighted_total_score": weighted_score,
        "decision": decision,
        "reason": reason,
        "reasoning": response_json.get("reasoning", ""),
        "pass": response_json.get("pass", False)
    }


def extract_json_from_text(text: str) -> Optional[Dict[str, Any]]:
    """
    ä»æ–‡æœ¬ä¸­æå– JSON å¯¹è±¡
    å¤„ç†å¯èƒ½åŒ…å«åœ¨ä»£ç å—æˆ–å…¶ä»–æ–‡æœ¬ä¸­çš„ JSON
    """
    # å°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # å°è¯•æå– JSON ä»£ç å—
    json_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
    match = re.search(json_block_pattern, text, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    # å°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { ... } å—
    brace_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    matches = re.findall(brace_pattern, text, re.DOTALL)
    for match in matches:
        try:
            return json.loads(match)
        except json.JSONDecodeError:
            continue
    
    return None


def flatten_json(json_obj: Dict[str, Any], prefix: str = "") -> Dict[str, Any]:
    """
    å°†åµŒå¥—çš„ JSON å¯¹è±¡å±•å¹³ä¸ºä¸€ç»´å­—å…¸
    ä¾‹å¦‚: {"clarity": 5, "accuracy": 4, "reasoning": "good"} 
    -> {"score_clarity": 5, "score_accuracy": 4, "eval_reasoning": "good"}
    """
    flattened = {}
    
    for key, value in json_obj.items():
        # å¤„ç† reasoning å­—æ®µ
        if key.lower() == "reasoning":
            new_key = config.REASONING_COLUMN_NAME
        else:
            # ä½¿ç”¨å‰ç¼€
            new_key = f"{config.EVAL_COLUMN_PREFIX}{key}"
        
        # å¦‚æœå€¼æ˜¯å­—å…¸ï¼Œé€’å½’å±•å¹³
        if isinstance(value, dict):
            nested = flatten_json(value, prefix=new_key + "_")
            flattened.update(nested)
        else:
            flattened[new_key] = value
    
    return flattened


def process_single_row(args: tuple) -> tuple:
    """å¤„ç†å•è¡Œæ•°æ®"""
    idx, row, llm_client = args
    
    try:
        # è·å–åŸå§‹æ–°é—»å’Œæ¨¡å‹è¾“å‡º
        original_text = str(row[config.INPUT_TEXT_COLUMN])
        model_output = str(row[config.FINAL_CONTENT_COLUMN])
        
        # è·³è¿‡é”™è¯¯è¡Œ
        if model_output.startswith("error:") or not model_output or model_output == "nan":
            return (idx, None, f"è·³è¿‡é”™è¯¯è¡Œæˆ–ç©ºå†…å®¹")
        
        # æ ¼å¼åŒ–è¯„ä¼°æç¤ºè¯
        eval_prompt = config.EVALUATION_PROMPT.format(
            original_text=original_text,
            model_output=model_output
        )
        
        # è°ƒç”¨ LLM è¿›è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨ç©ºç³»ç»Ÿæç¤ºè¯ï¼Œå› ä¸ºæç¤ºè¯å·²ç»åœ¨ eval_prompt ä¸­ï¼‰
        response = llm_client.generate(
            system_prompt="",
            user_prompt=eval_prompt
        )
        
        # æå– JSON
        json_obj = extract_json_from_text(response)
        
        if json_obj is None:
            return (idx, None, f"æ— æ³•ä»å“åº”ä¸­æå– JSON: {response[:100]}")
        
        # æå–å¹¶è®¡ç®—è¯„ä¼°ç»“æœ
        evaluation_result = extract_evaluation(json_obj)
        
        return (idx, evaluation_result, None)
    
    except Exception as e:
        return (idx, None, str(e))


def validate_csv_columns(df: pd.DataFrame, required_columns: list) -> None:
    """éªŒè¯ CSV æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—"""
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"CSV æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {', '.join(missing_columns)}\n"
                        f"å½“å‰ CSV æ–‡ä»¶çš„åˆ—: {', '.join(df.columns.tolist())}\n"
                        f"è¯·ç¡®ä¿ CSV æ–‡ä»¶åŒ…å«ä»¥ä¸‹åˆ—: {', '.join(required_columns)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºè¡Œ
    for col in required_columns:
        empty_rows = df[df[col].isna() | (df[col].astype(str).str.strip() == "")]
        if len(empty_rows) > 0:
            print(f"è­¦å‘Š: åˆ— '{col}' ä¸­æœ‰ {len(empty_rows)} è¡Œç©ºæ•°æ®ï¼Œè¿™äº›è¡Œå°†è¢«è·³è¿‡")


def batch_evaluate(input_csv: str, output_csv: str) -> None:
    """
    æ‰¹é‡è¯„ä¼°å“åº”
    
    Args:
        input_csv: è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ï¼ˆåº”åŒ…å« input_text å’Œ final_content åˆ—ï¼‰
        output_csv: è¾“å‡º CSV æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½ CSVï¼ˆå°è¯•å¤šç§ç¼–ç ï¼‰
    print(f"æ­£åœ¨åŠ è½½ CSV æ–‡ä»¶: {input_csv}")
    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1']
    df = None
    for enc in encodings:
        try:
            df = pd.read_csv(input_csv, encoding=enc)
            print(f"æˆåŠŸä½¿ç”¨ç¼–ç : {enc}")
            break
        except (UnicodeDecodeError, FileNotFoundError):
            continue
        except Exception as e:
            if "codec can't decode" not in str(e):
                raise
    
    if df is None:
        print(f"é”™è¯¯: æ— æ³•è¯»å– CSV æ–‡ä»¶ï¼Œå°è¯•äº†å¤šç§ç¼–ç éƒ½å¤±è´¥")
        sys.exit(1)
    
    # éªŒè¯å¿…éœ€çš„åˆ—
    required_columns = [config.INPUT_TEXT_COLUMN, config.FINAL_CONTENT_COLUMN]
    validate_csv_columns(df, required_columns)
    
    # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨è¯„ä¼°å™¨é…ç½®ï¼‰
    evaluator_provider = getattr(config, 'EVALUATOR_API_PROVIDER', config.API_PROVIDER)
    print(f"æ­£åœ¨åˆå§‹åŒ– {evaluator_provider.upper()} è¯„ä¼°å®¢æˆ·ç«¯...")
    try:
        # ä½¿ç”¨è¯„ä¼°å™¨ä¸“ç”¨çš„æ¨¡å‹é…ç½®
        if evaluator_provider.lower() == "deepseek":
            model = getattr(config, 'DEEPSEEK_REASONER_MODEL', config.DEEPSEEK_MODEL)
            temperature = getattr(config, 'DEEPSEEK_REASONER_TEMPERATURE', config.DEEPSEEK_TEMPERATURE)
            max_tokens = getattr(config, 'DEEPSEEK_REASONER_MAX_TOKENS', config.DEEPSEEK_MAX_TOKENS)
            top_p = getattr(config, 'DEEPSEEK_REASONER_TOP_P', None)
            llm_client = LLMClient(
                provider=evaluator_provider, 
                model=model, 
                temperature=temperature, 
                max_tokens=max_tokens,
                top_p=top_p
            )
        else:
            llm_client = LLMClient(provider=evaluator_provider)
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        sys.exit(1)
    
    # å‡†å¤‡æ•°æ®
    total_rows = len(df)
    print(f"æ€»å…±éœ€è¦å¤„ç† {total_rows} è¡Œæ•°æ®")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    results = {}
    errors = {}
    
    with ThreadPoolExecutor(max_workers=config.MAX_WORKERS) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(process_single_row, (idx, row, llm_client)): idx
            for idx, row in df.iterrows()
        }
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        with tqdm(total=total_rows, desc="è¯„ä¼°å“åº”") as pbar:
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    row_idx, flattened_dict, error = future.result()
                    if error:
                        errors[row_idx] = error
                        results[row_idx] = None
                    else:
                        results[row_idx] = flattened_dict
                except Exception as e:
                    errors[idx] = str(e)
                    results[idx] = None
                finally:
                    pbar.update(1)
    
    # åˆå§‹åŒ–è¯„ä¼°ç»“æœåˆ—
    eval_columns = [
        "eval_priority", "factuality_score", "completeness_score", 
        "adherence_score", "attractiveness_score", "weighted_total_score",
        "decision", "reason", "reasoning", "pass"
    ]
    for col in eval_columns:
        if col not in df.columns:
            df[col] = None
    
    # æ›´æ–° DataFrame
    for idx, evaluation_result in results.items():
        if evaluation_result:
            # ä¿å­˜è¯„ä¼°ç»“æœ
            df.at[idx, "eval_priority"] = evaluation_result.get("priority")
            df.at[idx, "factuality_score"] = evaluation_result.get("factuality_score")
            df.at[idx, "completeness_score"] = evaluation_result.get("completeness_score")
            df.at[idx, "adherence_score"] = evaluation_result.get("adherence_score")
            df.at[idx, "attractiveness_score"] = evaluation_result.get("attractiveness_score")
            df.at[idx, "weighted_total_score"] = evaluation_result.get("weighted_total_score")
            df.at[idx, "decision"] = evaluation_result.get("decision")
            df.at[idx, "reason"] = evaluation_result.get("reason")
            df.at[idx, "reasoning"] = evaluation_result.get("reasoning")
            df.at[idx, "pass"] = evaluation_result.get("pass")
        else:
            # æ ‡è®°é”™è¯¯
            error_msg = errors.get(idx, "æœªçŸ¥é”™è¯¯")
            df.at[idx, "decision"] = "ERROR"
            df.at[idx, "reason"] = f"error: {error_msg}"
    
    # ä¿å­˜ç»“æœ
    print(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_csv}")
    df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    
    # ç»Ÿè®¡ä¿¡æ¯
    success_count = sum(1 for v in results.values() if v is not None)
    error_count = total_rows - success_count
    print(f"\nå¤„ç†å®Œæˆ!")
    print(f"æˆåŠŸ: {success_count} è¡Œ")
    print(f"é”™è¯¯: {error_count} è¡Œ")
    
    if errors:
        print("\né”™è¯¯è¯¦æƒ…:")
        for idx, error_msg in errors.items():
            print(f"  è¡Œ {idx}: {error_msg}")
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("è¯„ä¼°ç»“æœç»Ÿè®¡")
    print("=" * 60)
    
    if "decision" in df.columns:
        decision_counts = df["decision"].value_counts()
        print(f"\nğŸ“Š å†³ç­–åˆ†å¸ƒ:")
        for decision, count in decision_counts.items():
            percentage = (count / total_rows) * 100
            print(f"  {decision:12s}: {count:4d} æ¡ ({percentage:5.1f}%)")
    
    if "weighted_total_score" in df.columns:
        valid_scores = df[df["weighted_total_score"].notna()]["weighted_total_score"]
        if len(valid_scores) > 0:
            print(f"\nğŸ“ˆ åŠ æƒæ€»åˆ†ç»Ÿè®¡ (0-100 åˆ†åˆ¶):")
            print(f"  å¹³å‡åˆ†: {valid_scores.mean():.2f}")
            print(f"  æœ€é«˜åˆ†: {valid_scores.max():.2f}")
            print(f"  æœ€ä½åˆ†: {valid_scores.min():.2f}")
            print(f"  ä¸­ä½æ•°: {valid_scores.median():.2f}")
            
            # åˆ†æ•°åˆ†å¸ƒ
            high_quality = len(valid_scores[valid_scores >= 75])
            medium_quality = len(valid_scores[(valid_scores >= 60) & (valid_scores < 75)])
            low_quality = len(valid_scores[valid_scores < 60])
            print(f"\n  åˆ†æ•°åˆ†å¸ƒ:")
            print(f"    é«˜è´¨é‡ (â‰¥75åˆ†): {high_quality:4d} æ¡ ({high_quality/len(valid_scores)*100:5.1f}%)")
            print(f"    ä¸­ç­‰è´¨é‡ (60-74åˆ†): {medium_quality:4d} æ¡ ({medium_quality/len(valid_scores)*100:5.1f}%)")
            print(f"    ä½è´¨é‡ (<60åˆ†): {low_quality:4d} æ¡ ({low_quality/len(valid_scores)*100:5.1f}%)")
    
    # å„ç»´åº¦è¯„åˆ†ç»Ÿè®¡
    score_columns = ["factuality_score", "completeness_score", "adherence_score", "attractiveness_score"]
    available_score_columns = [col for col in score_columns if col in df.columns]
    if available_score_columns:
        print(f"\nğŸ“‹ å„ç»´åº¦è¯„åˆ†ç»Ÿè®¡ (0-10 åˆ†åˆ¶):")
        for col in available_score_columns:
            valid_scores = df[df[col].notna()][col]
            if len(valid_scores) > 0:
                col_name = col.replace("_score", "").replace("_", " ").title()
                print(f"  {col_name:20s}: å¹³å‡ {valid_scores.mean():.2f}, æœ€é«˜ {valid_scores.max():.2f}, æœ€ä½ {valid_scores.min():.2f}")
    
    # ä¼˜å…ˆçº§åˆ†å¸ƒ
    if "eval_priority" in df.columns:
        priority_counts = df["eval_priority"].value_counts()
        if len(priority_counts) > 0:
            print(f"\nğŸ·ï¸  ä¼˜å…ˆçº§åˆ†å¸ƒ:")
            for priority, count in priority_counts.items():
                if pd.notna(priority):
                    percentage = (count / total_rows) * 100
                    print(f"  {priority:12s}: {count:4d} æ¡ ({percentage:5.1f}%)")
    
    print(f"\nâœ… åˆ›å»ºçš„è¯„ä¼°åˆ—: {', '.join(eval_columns)}")
    print("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python batch_evaluator.py <è¾“å…¥CSVæ–‡ä»¶> <è¾“å‡ºCSVæ–‡ä»¶>")
        print("ç¤ºä¾‹: python batch_evaluator.py output.csv evaluated.csv")
        sys.exit(1)
    
    input_csv = sys.argv[1]
    output_csv = sys.argv[2]
    
    batch_evaluate(input_csv, output_csv)


if __name__ == "__main__":
    main()
