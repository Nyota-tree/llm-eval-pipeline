# -*- coding: utf-8 -*-
"""中英文界面文案"""
import streamlit as st

TEXTS = {
    "zh": {
        "app_title": "LLM 评测流水线",
        "breadcrumb": "配置 → 业务 Prompt → 评估 Prompt → 生成回答 → 评测 → 结果展示",
        "sidebar_config": "配置",
        "language": "语言",
        "api_key": "API Key",
        "api_key_help": "DeepSeek API Key",
        "model": "Model",
        "model_help": "评测与生成使用的模型",
        "data_template": "数据模板",
        "download_csv_template": "下载 CSV 模板",
        "restart": "重新开始",
        "phase1_title": "阶段一：场景定义与上传",
        "scenario_caption": "业务场景（请尽量详细描述，越详细生成的 Prompt 越精准）",
        "scenario_label": "业务场景",
        "scenario_placeholder": "请详细描述：\n· 该 AI 产品的具体用途、目标用户画像\n· 典型使用场景与边界情况\n· 希望的语气、风格或合规要求\n\n例如：面向 6–10 岁儿童的故事生成助手……",
        "scenario_help": "描述越详细，大模型生成的业务提示词越贴合你的需求。",
        "north_star_label": "北极星指标",
        "north_star_placeholder": "例如：趣味性、符合儿童心智、专业度、安全性",
        "north_star_help": "衡量该 AI 表现好坏的核心业务标准，可写多条。",
        "upload_csv": "上传评测数据（仅限 CSV）",
        "upload_help": "需包含 question 列；可选 expected_answer（有则可不生成直接评测）",
        "preview": "预览（前 3 行）",
        "next_generate_prompt": "下一步：生成 Prompt",
        "has_answer_btn": "已有回答，直接生成评测方案",
        "err_fill_api_key": "请在侧边栏填写 API Key。",
        "err_fill_scenario": "请填写业务场景和北极星指标。",
        "err_upload_csv": "请先上传包含 question 的 CSV 文件。",
        "err_config_incomplete": "配置或数据不完整，请返回上一步。（请确认侧边栏已填 API Key 且已上传含 question 的 CSV）",
        "err_config_incomplete_no_prompt": "配置或数据不完整，请返回上一步。（当前有题目尚无回答，需在阶段二配置「业务 Prompt」后再进入本阶段；或为这些题目在上传的 CSV 中填写 expected_answer）",
        "err_file_decode": "文件解析失败：{msg}. 请使用 UTF-8 或 GBK 编码的 CSV。",
        "success_eval_generated": "评测方案已生成，请确认并编辑下方提示词。",
        "success_gen_prompt": "评估 Prompt 已生成，请确认并编辑下方提示词，再开始处理数据。",
        "phase2_title": "阶段二：业务 Prompt 确认",
        "phase2_caption": "根据业务场景与北极星指标已生成下方业务提示词，可编辑。确认后进入下一步生成「评估 Prompt」。",
        "business_prompt_label": "业务 Prompt（可编辑）",
        "business_prompt_help": "用于调用模型生成回答的系统提示词，每条题目将作为用户输入传入。",
        "next_generate_eval": "下一步：生成评估 Prompt",
        "back_config": "返回配置",
        "err_fill_business_prompt": "请填写或保留业务 Prompt。",
        "phase3_title": "阶段三：评估 Prompt 确认",
        "phase3_caption": "上一步已生成评估用提示词，可编辑。确认后进入「处理数据」：先生成回答，再执行评测。",
        "eval_prompt_label": "评估 Prompt（可编辑）",
        "eval_prompt_help": "可根据需要修改生成的评测标准；须包含占位符 {original_text} 与 {model_output}。",
        "placeholder_warning": "提示词中建议包含占位符 `{original_text}` 与 `{model_output}`，以便对每条题目进行评测。",
        "confirm_start": "确认并开始处理数据",
        "back_business_prompt": "返回业务 Prompt",
        "err_fill_eval_prompt": "请填写或保留评估提示词。",
        "phase4_title": "阶段四：生成回答",
        "return_eval_prompt": "返回评估 Prompt",
        "caption_has_answers": "数据中已包含回答（generated_answer 或 expected_answer），可直接点击下方「下一步：开始评测」。",
        "gen_result_title": "生成结果（只读）",
        "gen_result_caption": "以下为将用于评测的回答（生成回答或上传的预期回答）。确认后点击「下一步：开始评测」。",
        "next_start_eval": "下一步：开始评测",
        "phase5_title": "阶段五：执行评测",
        "err_api_key": "请先在侧边栏填写 API Key。",
        "err_no_data": "无有效数据，请返回上传 CSV。",
        "success_eval_done": "共评测 {n} 条，耗时 {elapsed:.1f} 秒。",
        "phase6_title": "阶段六：结果展示",
        "no_results": "暂无结果，请先完成评测。",
        "core_metrics": "核心指标",
        "avg_score": "平均分",
        "pass_rate": "通过率",
        "error_count": "错误条数",
        "total_count": "总条数",
        "elapsed": "总耗时",
        "score_dist": "得分分布",
        "score_dist_title": "加权总分分布",
        "score_x": "加权总分",
        "score_y": "条数",
        "no_valid_scores": "无有效数值得分，跳过得分分布图。",
        "expander_diff_title": "分数集中、缺乏差异度？",
        "expander_diff_caption": "若多数样本得分接近，可在「评估 Prompt」中要求评委严格区分档次（如 90+ / 80–89 / 70–79），并明确各分数段对应的表现描述，避免扎堆打高分。修改后重新跑评测即可。",
        "prompts_section": "本次使用的 Prompt（供核查）",
        "expand_business_prompt": "业务 Prompt（生成回答时使用）",
        "expand_eval_prompt": "评估 Prompt（评测时使用）",
        "full_results_caption": "完整结果（含原题、回答、各维度小分、总分、决策与理由）",
        "reject_note": "说明：REJECT 表示「事实性/安全性」分数低于阈值（0–10 分制低于 5 分，或 0–100 分制低于 50 分），与总分无关。",
        "download_csv": "下载完整结果 CSV",
        "col_question": "题目",
        "col_answer": "回答",
        "col_factuality": "事实性/安全性",
        "col_north_star": "北极星指标",
        "col_completeness": "完整性与连贯性",
        "col_weighted_total": "加权总分",
        "progress_preparing": "准备中…",
        "status_generating": "生成回答中…",
        "status_evaluating": "评测进行中…",
        "skip_empty_question": "题目为空，已跳过",
        "gen_ok": "已生成",
        "eval_ok": "得分",
    },
    "en": {
        "app_title": "LLM Eval Pipeline",
        "breadcrumb": "Config → Gen Prompt → Eval Prompt → Generate → Evaluate → Results",
        "sidebar_config": "Settings",
        "language": "Language",
        "api_key": "API Key",
        "api_key_help": "DeepSeek API Key",
        "model": "Model",
        "model_help": "Model used for both generation and evaluation",
        "data_template": "Data template",
        "download_csv_template": "Download CSV template",
        "restart": "Restart",
        "phase1_title": "Step 1: Scenario & upload",
        "scenario_caption": "Business scenario (the more detail, the better the generated prompt)",
        "scenario_label": "Business scenario",
        "scenario_placeholder": "Describe: use case, target users, tone, constraints…",
        "scenario_help": "More detail helps the model generate a better prompt.",
        "north_star_label": "North star metric",
        "north_star_placeholder": "e.g. fun, child-friendly, safety, professionalism",
        "north_star_help": "Core criteria for good outputs. You can list multiple.",
        "upload_csv": "Upload CSV (eval data)",
        "upload_help": "Must include 'question' column; optional 'expected_answer' to skip generation.",
        "preview": "Preview (first 3 rows)",
        "next_generate_prompt": "Next: Generate prompts",
        "has_answer_btn": "I have answers, generate eval plan only",
        "err_fill_api_key": "Please enter API Key in the sidebar.",
        "err_fill_scenario": "Please fill in scenario and north star metric.",
        "err_upload_csv": "Please upload a CSV with a 'question' column first.",
        "err_config_incomplete": "Config or data incomplete. Go back and check API Key and CSV with 'question'.",
        "err_config_incomplete_no_prompt": "Config or data incomplete. Either set up business prompt in Step 2 or add expected_answer in CSV.",
        "err_file_decode": "File decode failed: {msg}. Use UTF-8 or GBK encoding.",
        "success_eval_generated": "Eval plan generated. Review and edit the prompt below.",
        "success_gen_prompt": "Eval prompt generated. Review and edit, then start processing.",
        "phase2_title": "Step 2: Business prompt",
        "phase2_caption": "Edit the business prompt below, then generate the eval prompt.",
        "business_prompt_label": "Business prompt (editable)",
        "business_prompt_help": "System prompt used to generate answers for each question.",
        "next_generate_eval": "Next: Generate eval prompt",
        "back_config": "Back to config",
        "err_fill_business_prompt": "Please enter or keep the business prompt.",
        "phase3_title": "Step 3: Eval prompt",
        "phase3_caption": "Edit the eval prompt below. Then confirm to generate answers and run evaluation.",
        "eval_prompt_label": "Eval prompt (editable)",
        "eval_prompt_help": "Must include placeholders {original_text} and {model_output}.",
        "placeholder_warning": "Prompt should include placeholders {original_text} and {model_output}.",
        "confirm_start": "Confirm and start processing",
        "back_business_prompt": "Back to business prompt",
        "err_fill_eval_prompt": "Please enter or keep the eval prompt.",
        "phase4_title": "Step 4: Generate answers",
        "return_eval_prompt": "Back to eval prompt",
        "caption_has_answers": "Data already has answers. Click below to go to evaluation.",
        "gen_result_title": "Generated results (read-only)",
        "gen_result_caption": "Answers to be evaluated. Click below to start evaluation.",
        "next_start_eval": "Next: Start evaluation",
        "phase5_title": "Step 5: Evaluation",
        "err_api_key": "Please enter API Key in the sidebar.",
        "err_no_data": "No valid data. Please upload a CSV.",
        "success_eval_done": "Evaluated {n} items in {elapsed:.1f}s.",
        "phase6_title": "Step 6: Results",
        "no_results": "No results yet. Run evaluation first.",
        "core_metrics": "Core metrics",
        "avg_score": "Avg score",
        "pass_rate": "Pass rate",
        "error_count": "Errors",
        "total_count": "Total",
        "elapsed": "Time (s)",
        "score_dist": "Score distribution",
        "score_dist_title": "Weighted total score",
        "score_x": "Weighted total",
        "score_y": "Count",
        "no_valid_scores": "No valid scores; skipping chart.",
        "expander_diff_title": "Scores too similar?",
        "expander_diff_caption": "If most scores are the same, ask the eval prompt to differentiate tiers (e.g. 90+ / 80–89 / 70–79) and avoid clustering at one score.",
        "prompts_section": "Prompts used (for reference)",
        "expand_business_prompt": "Business prompt (for generation)",
        "expand_eval_prompt": "Eval prompt (for evaluation)",
        "full_results_caption": "Full results (question, answer, scores, decision, reason)",
        "reject_note": "REJECT = factuality/safety below threshold (e.g. <50 on 0–100 scale).",
        "download_csv": "Download full results CSV",
        "col_question": "Question",
        "col_answer": "Answer",
        "col_factuality": "Factuality / Safety",
        "col_north_star": "North star",
        "col_completeness": "Completeness & Coherence",
        "col_weighted_total": "Weighted total",
        "progress_preparing": "Preparing…",
        "status_generating": "Generating answers…",
        "status_evaluating": "Evaluating…",
        "skip_empty_question": "Empty question, skipped",
        "gen_ok": "Generated",
        "eval_ok": "Score",
    },
}


def t(key: str, **kwargs) -> str:
    """Get translated string for current language. Uses st.session_state.lang if available."""
    try:
        lang = st.session_state.get("lang", "zh")
    except Exception:
        lang = "zh"
    s = TEXTS.get(lang, TEXTS["zh"]).get(key, TEXTS["zh"].get(key, key))
    if kwargs:
        s = s.format(**kwargs)
    return s
