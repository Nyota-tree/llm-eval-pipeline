"""
LLM è¯„æµ‹æµæ°´çº¿ - Streamlit åº”ç”¨
å…­é˜¶æ®µæµç¨‹ï¼šé…ç½® â†’ ä¸šåŠ¡ Prompt â†’ è¯„ä¼° Prompt â†’ ç”Ÿæˆå›žç­” â†’ è¯„æµ‹ â†’ ç»“æžœå±•ç¤º
"""

import io
import os
import time
import re
import json
from typing import Any, Dict, Optional

import pandas as pd
import streamlit as st
import plotly.express as px

# é¡¹ç›®æ¨¡å—ï¼ˆéœ€åœ¨è¿è¡Œæ—¶å¯ç”¨ï¼‰
import config
from utils import LLMClient
from generate_evaluator_prompt import PROMPT_GENERATOR_SYSTEM_PROMPT
from batch_evaluator import extract_json_from_text, extract_evaluation
from i18n import t


def generate_evaluator_prompt_in_app(scenario: str, north_star_metric: str, api_key: str) -> str:
    """åœ¨åº”ç”¨å†…ç”Ÿæˆè¯„æµ‹æ–¹æ¡ˆï¼ˆä¸è°ƒç”¨ sys.exitï¼Œä¾¿äºŽ Streamlit å±•ç¤ºé”™è¯¯ï¼‰ã€‚"""
    user_prompt = f"""åœºæ™¯ï¼š{scenario}
åŒ—æžæ˜ŸæŒ‡æ ‡ï¼š{north_star_metric}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°å‘˜ Promptã€‚"""
    prev = os.environ.get("DEEPSEEK_API_KEY")
    try:
        os.environ["DEEPSEEK_API_KEY"] = api_key
        llm_client = LLMClient(provider="deepseek", model=st.session_state.get("model", DEFAULT_MODEL))
        return llm_client.generate(system_prompt=PROMPT_GENERATOR_SYSTEM_PROMPT, user_prompt=user_prompt)
    finally:
        if prev is not None:
            os.environ["DEEPSEEK_API_KEY"] = prev
        else:
            os.environ.pop("DEEPSEEK_API_KEY", None)


def generate_generation_prompt_in_app(scenario: str, north_star: str, api_key: str) -> str:
    """æ ¹æ®ä¸šåŠ¡åœºæ™¯ä¸ŽåŒ—æžæ˜ŸæŒ‡æ ‡ï¼Œè°ƒç”¨å¤§æ¨¡åž‹ç”Ÿæˆé«˜è´¨é‡çš„ã€Œç”Ÿæˆ Promptã€ã€‚"""
    user_prompt = f"""ä¸šåŠ¡åœºæ™¯ï¼š
{scenario}

åŒ—æžæ˜ŸæŒ‡æ ‡ï¼š
{north_star}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¸€ä»½å¯ç›´æŽ¥ç”¨äºŽç”Ÿäº§çŽ¯å¢ƒçš„ä¸šåŠ¡æç¤ºè¯ï¼ˆä»…è¾“å‡ºæç¤ºè¯æ­£æ–‡ï¼Œæ— éœ€é¢å¤–è¯´æ˜Žï¼‰ã€‚"""
    prev = os.environ.get("DEEPSEEK_API_KEY")
    try:
        os.environ["DEEPSEEK_API_KEY"] = api_key
        llm_client = LLMClient(provider="deepseek", model=st.session_state.get("model", DEFAULT_MODEL))
        return llm_client.generate(
            system_prompt=GENERATION_PROMPT_GENERATOR_SYSTEM,
            user_prompt=user_prompt,
        )
    finally:
        if prev is not None:
            os.environ["DEEPSEEK_API_KEY"] = prev
        else:
            os.environ.pop("DEEPSEEK_API_KEY", None)


# ==================== é¡µé¢é…ç½® ====================
st.set_page_config(
    page_title="LLM è¯„æµ‹æµæ°´çº¿",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ==================== å¸¸é‡ ====================
REQUIRED_CSV_COLUMNS = ["question"]
OPTIONAL_ANSWER_COLUMN = "expected_answer"
GENERATED_ANSWER_COLUMN = "generated_answer"
DEFAULT_MODEL = "deepseek-chat"
MODEL_OPTIONS = ["deepseek-chat", "deepseek-reasoner"]
PHASES = ["CONFIG", "GENERATION_PROMPT_EDIT", "GENERATING", "PROMPT_EDIT", "EVALUATING", "RESULT"]

# è°ƒç”¨å¤§æ¨¡åž‹ç”Ÿæˆã€Œç”Ÿæˆ Promptã€æ—¶ä½¿ç”¨çš„ç³»ç»Ÿæç¤ºè¯
GENERATION_PROMPT_GENERATOR_SYSTEM = """# Role
ä½ æ˜¯ä¸€ä½ä¸–ç•Œçº§çš„æç¤ºè¯å·¥ç¨‹å¸ˆï¼Œæ“…é•¿å°†ä¸šåŠ¡éœ€æ±‚è½¬åŒ–ä¸ºæžå…·æ‰§è¡ŒåŠ›çš„ LLM ç”Ÿäº§çŽ¯å¢ƒæç¤ºè¯ã€‚ä½ æ·±è°™æç¤ºè¯å·¥ç¨‹çš„æœ€ä½³å®žè·µï¼Œèƒ½å¤Ÿé’ˆå¯¹ç‰¹å®šä¸šåŠ¡æŒ‡æ ‡è¿›è¡Œç²¾ç»†åŒ–è°ƒä¼˜ã€‚

# Task
æ ¹æ®ç”¨æˆ·æä¾›çš„ã€ä¸šåŠ¡åœºæ™¯ã€‘å’Œã€åŒ—æžæ˜ŸæŒ‡æ ‡ã€‘ï¼Œä¸ºç”¨æˆ·æ’°å†™ä¸€ä¸ªé«˜è´¨é‡çš„ã€å¯ç›´æŽ¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨çš„"ä¸šåŠ¡æç¤ºè¯"ã€‚

# Input
1. ä¸šåŠ¡åœºæ™¯ï¼šæè¿°è¯¥ AI äº§å“çš„å…·ä½“ç”¨é€”å’Œç”¨æˆ·ç”»åƒã€‚
2. åŒ—æžæ˜ŸæŒ‡æ ‡ï¼šè¡¡é‡è¯¥ AI è¡¨çŽ°å¥½åçš„æ ¸å¿ƒä¸šåŠ¡æ ‡å‡†ã€‚

# Output Framework (ä½ ç”Ÿæˆçš„ä¸šåŠ¡æç¤ºè¯å¿…é¡»åŒ…å«)
1. **è§’è‰²è®¾å®š (Role)**ï¼šå®šä¹‰ä¸€ä¸ªç¬¦åˆåœºæ™¯çš„ã€ä¸“ä¸šçš„ã€å…·æœ‰ç‰¹å®šäººæ ¼ç‰¹è´¨çš„ AI è§’è‰²ã€‚
2. **ä»»åŠ¡æè¿° (Task)**ï¼šæ¸…æ™°ã€æ— æ­§ä¹‰åœ°æè¿° AI éœ€è¦å®Œæˆçš„å…·ä½“å·¥ä½œã€‚
3. **çº¦æŸæ¡ä»¶ (Constraints)**ï¼š
    - æ ¹æ®ã€åŒ—æžæ˜ŸæŒ‡æ ‡ã€‘åæŽ¨çš„ç¡¬æ€§è¦æ±‚ï¼ˆä¾‹å¦‚ï¼šä¸¥ç¦èƒ¡è¯´å…«é“ã€å›žå¤é•¿åº¦é™åˆ¶ã€å¿…é¡»åŒ…å«æŸäº›å…³é”®è¯ï¼‰ã€‚
    - é’ˆå¯¹ã€ä¸šåŠ¡åœºæ™¯ã€‘çš„åˆè§„æ€§è¦æ±‚æˆ–è¯­æ°”è¦æ±‚ã€‚
4. **æ€ç»´é“¾è¦æ±‚ (Chain of Thought)**ï¼šå¼•å¯¼ AI åœ¨è¾“å‡ºç»“æžœå‰è¿›è¡Œå†…éƒ¨æŽ¨ç†ï¼Œä»¥ç¡®ä¿é€»è¾‘ä¸¥å¯†ï¼ˆå¦‚æžœåœºæ™¯å¤æ‚ï¼‰ã€‚
5. **è¾“å‡ºæ ¼å¼ (Output Format)**ï¼šå®šä¹‰å›žå¤çš„ç»“æž„ï¼ˆå¦‚ Markdownã€JSON æˆ–ç‰¹å®šè¯­æ°”çš„æ–‡æœ¬ï¼‰ã€‚

# Design Principles
- **æŒ‡æ ‡å¯¹é½**ï¼šå¦‚æžœåŒ—æžæ˜ŸæŒ‡æ ‡æ˜¯"ä¸“ä¸šåº¦"ï¼Œæç¤ºè¯åº”ä¾§é‡å¼•ç”¨çŸ¥è¯†åº“å’Œæœ¯è¯­ï¼›å¦‚æžœæ˜¯"äº²å’ŒåŠ›"ï¼Œåˆ™ä¾§é‡è¯­æ°”è¯å’Œå…±æƒ…è¡¨è¾¾ã€‚
- **æ¨¡å—åŒ–**ï¼šç»“æž„æ¸…æ™°ï¼Œç”¨æˆ·æ‹¿åˆ°åŽå¯ä»¥ä¸€çœ¼çœ‹æ‡‚æ¯ä¸€éƒ¨åˆ†çš„ä½œç”¨ã€‚
- **é²æ£’æ€§**ï¼šè€ƒè™‘åˆ°å¤§æ¨¡åž‹çš„è¾¹ç•Œæƒ…å†µï¼Œå¢žåŠ é¢„é˜²è¯¯æ“ä½œæˆ–è¶Šæƒçš„é˜²å¾¡æ€§æè¿°ã€‚

# Language
å§‹ç»ˆä½¿ç”¨ä¸­æ–‡è¾“å‡ºç”Ÿæˆçš„ä¸šåŠ¡æç¤ºè¯ã€‚"""


def init_session_state():
    """åˆå§‹åŒ– session_state"""
    if "phase" not in st.session_state:
        st.session_state.phase = "CONFIG"
    if "lang" not in st.session_state:
        st.session_state.lang = "zh"
    if "api_key" not in st.session_state:
        st.session_state.api_key = ""
    if "model" not in st.session_state:
        st.session_state.model = DEFAULT_MODEL
    if "scenario" not in st.session_state:
        st.session_state.scenario = ""
    if "north_star" not in st.session_state:
        st.session_state.north_star = ""
    if "uploaded_df" not in st.session_state:
        st.session_state.uploaded_df = None
    if "generation_prompt" not in st.session_state:
        st.session_state.generation_prompt = ""
    if "generated_prompt" not in st.session_state:
        st.session_state.generated_prompt = ""
    if "evaluation_prompt" not in st.session_state:
        st.session_state.evaluation_prompt = ""
    if "results_df" not in st.session_state:
        st.session_state.results_df = None
    if "eval_elapsed" not in st.session_state:
        st.session_state.eval_elapsed = None


def get_csv_template_bytes() -> bytes:
    """ç”Ÿæˆç¤ºä¾‹ CSV æ¨¡æ¿ï¼ˆä»… questionï¼Œç”¨äºŽã€Œç”Ÿæˆå›žç­”ã€æµç¨‹ï¼‰"""
    template_df = pd.DataFrame({
        "question": [
            "ç¤ºä¾‹é—®é¢˜ 1ï¼šè¯·ç®€è¿°åˆè§„è¦ç‚¹",
            "ç¤ºä¾‹é—®é¢˜ 2ï¼šè¯¥åœºæ™¯ä¸‹åº”å¦‚ä½•å›žå¤å®¢æˆ·ï¼Ÿ",
        ],
    })
    buf = io.BytesIO()
    template_df.to_csv(buf, index=False, encoding="utf-8-sig")
    return buf.getvalue()


def validate_csv(df: pd.DataFrame) -> tuple[bool, str]:
    """éªŒè¯ CSV æ˜¯å¦åŒ…å«å¿…éœ€åˆ—ã€‚è¿”å›ž (æ˜¯å¦é€šè¿‡, é”™è¯¯ä¿¡æ¯)ã€‚"""
    missing = [c for c in REQUIRED_CSV_COLUMNS if c not in df.columns]
    if missing:
        return False, f"CSV ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing)}ã€‚éœ€è¦: {', '.join(REQUIRED_CSV_COLUMNS)}"
    if df.empty:
        return False, "CSV ä¸ºç©ºï¼Œè¯·ä¸Šä¼ è‡³å°‘åŒ…å«ä¸€è¡Œçš„æ•°æ®ã€‚"
    return True, ""


def run_single_generation(
    question: str,
    system_prompt: str,
    api_key: str,
    model: str,
) -> tuple[Optional[str], Optional[str]]:
    """ä½¿ç”¨ç»™å®šçš„ç”Ÿæˆ Prompt å¯¹å•æ¡é¢˜ç›®ç”Ÿæˆå›žç­”ã€‚è¿”å›ž (å›žç­”æ–‡æœ¬, é”™è¯¯ä¿¡æ¯)ã€‚"""
    if not (question or "").strip():
        return None, "é¢˜ç›®ä¸ºç©º"
    if not (system_prompt or "").strip():
        return None, "ç”Ÿæˆ Prompt ä¸ºç©º"
    prev = os.environ.get("DEEPSEEK_API_KEY")
    try:
        os.environ["DEEPSEEK_API_KEY"] = api_key
        client = LLMClient(
            provider="deepseek",
            model=model,
            temperature=getattr(config, "DEEPSEEK_TEMPERATURE", 0.7),
            max_tokens=getattr(config, "DEEPSEEK_MAX_TOKENS", 4000),
        )
        answer = client.generate(system_prompt=system_prompt.strip(), user_prompt=(question or "").strip())
        return (answer or "").strip(), None
    except Exception as e:
        return None, str(e)
    finally:
        if prev is not None:
            os.environ["DEEPSEEK_API_KEY"] = prev
        else:
            os.environ.pop("DEEPSEEK_API_KEY", None)


def _fill_evaluation_prompt(prompt: str, original_text: str, model_output: str) -> str:
    """ä»…æ›¿æ¢ {original_text} ä¸Ž {model_output}ï¼Œé¿å… JSON ç­‰èŠ±æ‹¬å·è¢« format è¯¯è§£æžã€‚"""
    return prompt.replace("{original_text}", original_text).replace("{model_output}", model_output)


def run_single_evaluation(
    row: pd.Series,
    evaluation_prompt: str,
    api_key: str,
    model: str,
) -> tuple[Optional[Dict[str, Any]], Optional[str]]:
    """å¯¹å•è¡Œæ‰§è¡Œè¯„æµ‹ã€‚model_output ä¼˜å…ˆç”¨ generated_answerï¼Œå¦åˆ™ expected_answerã€‚"""
    original_text = str(row.get("question", ""))
    model_output = str(
        row.get(GENERATED_ANSWER_COLUMN) or row.get(OPTIONAL_ANSWER_COLUMN) or ""
    ).strip()
    if not original_text.strip():
        return None, "é—®é¢˜ä¸ºç©ºï¼Œå·²è·³è¿‡"
    if not model_output or model_output.lower() in ("nan", ""):
        return None, "è¯¥è¡Œæ— å›žç­”å†…å®¹ï¼ˆéœ€å…ˆã€Œç”Ÿæˆå›žç­”ã€æˆ–ä¸Šä¼ å¸¦ expected_answer çš„ CSVï¼‰ï¼Œå·²è·³è¿‡"

    prompt_filled = _fill_evaluation_prompt(evaluation_prompt, original_text, model_output)
    prev_key = os.environ.get("DEEPSEEK_API_KEY")
    try:
        os.environ["DEEPSEEK_API_KEY"] = api_key
        client = LLMClient(
            provider="deepseek",
            model=model,
            temperature=getattr(config, "DEEPSEEK_REASONER_TEMPERATURE", 0.0),
            max_tokens=getattr(config, "DEEPSEEK_REASONER_MAX_TOKENS", 2000),
            top_p=getattr(config, "DEEPSEEK_REASONER_TOP_P", None),
        )
        response = client.generate(system_prompt="", user_prompt=prompt_filled)
    except Exception as e:
        return None, str(e)
    finally:
        if prev_key is not None:
            os.environ["DEEPSEEK_API_KEY"] = prev_key
        else:
            os.environ.pop("DEEPSEEK_API_KEY", None)

    json_obj = extract_json_from_text(response)
    if json_obj is None:
        return None, f"æ— æ³•ä»Žå“åº”ä¸­æå– JSON: {response[:200]}â€¦"

    try:
        evaluation_result = extract_evaluation(json_obj)
        return evaluation_result, None
    except Exception as e:
        return None, str(e)


# ==================== ä¾§è¾¹æ  ====================
def render_sidebar():
    with st.sidebar:
        st.header("âš™ï¸ " + t("sidebar_config"))
        st.divider()

        lang = st.selectbox(
            t("language"),
            options=["zh", "en"],
            format_func=lambda x: "ä¸­æ–‡" if x == "zh" else "English",
            index=0 if st.session_state.get("lang", "zh") == "zh" else 1,
        )
        st.session_state.lang = lang

        api_key = st.text_input(
            t("api_key"),
            value=st.session_state.get("api_key", ""),
            type="password",
            placeholder="sk-â€¦",
            help=t("api_key_help"),
        )
        st.session_state.api_key = api_key

        model = st.selectbox(
            t("model"),
            options=MODEL_OPTIONS,
            index=MODEL_OPTIONS.index(st.session_state.get("model", DEFAULT_MODEL)),
            help=t("model_help"),
        )
        st.session_state.model = model

        st.divider()
        st.caption(t("data_template"))
        template_bytes = get_csv_template_bytes()
        st.download_button(
            label=t("download_csv_template"),
            data=template_bytes,
            file_name="eval_template.csv",
            mime="text/csv",
        )
        st.divider()

        if st.button("ðŸ”„ " + t("restart"), width="stretch"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            init_session_state()
            st.rerun()


# ==================== Phase 1: é…ç½®ä¸Žä¸Šä¼  ====================
def render_phase_config():
    st.subheader(t("phase1_title"))
    st.divider()

    st.caption(t("scenario_caption"))
    scenario = st.text_area(
        t("scenario_label"),
        value=st.session_state.scenario,
        height=220,
        placeholder=t("scenario_placeholder"),
        help=t("scenario_help"),
    )
    st.session_state.scenario = scenario

    st.divider()
    north_star = st.text_input(
        t("north_star_label"),
        value=st.session_state.north_star,
        placeholder=t("north_star_placeholder"),
        help=t("north_star_help"),
    )
    st.session_state.north_star = north_star

    st.divider()
    uploaded = st.file_uploader(t("upload_csv"), type=["csv"], help=t("upload_help"))

    if uploaded is not None:
        df = None
        last_err = None
        for enc in ("utf-8", "utf-8-sig", "gbk", "gb2312", "latin-1"):
            try:
                uploaded.seek(0)
                df = pd.read_csv(uploaded, encoding=enc)
                break
            except UnicodeDecodeError:
                continue
            except Exception as e:
                last_err = e
                break
        if df is None:
            st.error(t("err_file_decode", msg=last_err or "æ— æ³•è¯†åˆ«çš„ç¼–ç "))
            return
        ok, err = validate_csv(df)
        if not ok:
            st.error(err)
            return
        st.session_state.uploaded_df = df
        st.caption(t("preview"))
        st.dataframe(df.head(3), width="stretch", hide_index=True)

    st.divider()
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        if st.button(t("next_generate_prompt"), type="primary", width="stretch"):
            if not st.session_state.api_key.strip():
                st.error(t("err_fill_api_key"))
            elif not st.session_state.scenario.strip() or not st.session_state.north_star.strip():
                st.error(t("err_fill_scenario"))
            elif st.session_state.uploaded_df is None or st.session_state.uploaded_df.empty:
                st.error(t("err_upload_csv"))
            else:
                with st.spinner("æ­£åœ¨è°ƒç”¨å¤§æ¨¡åž‹ç”Ÿæˆä¸šåŠ¡æç¤ºè¯â€¦"):
                    try:
                        st.session_state.generation_prompt = generate_generation_prompt_in_app(
                            st.session_state.scenario,
                            st.session_state.north_star,
                            st.session_state.api_key,
                        )
                        st.session_state.phase = "GENERATION_PROMPT_EDIT"
                        st.rerun()
                    except Exception as e:
                        st.error(f"ç”Ÿæˆ Prompt å¤±è´¥ï¼ˆè¯·æ£€æŸ¥ API Key ä¸Žç½‘ç»œï¼‰ï¼š{e}")
    with col_btn2:
        has_answer = (
            st.session_state.uploaded_df is not None
            and not st.session_state.uploaded_df.empty
            and (
                OPTIONAL_ANSWER_COLUMN in st.session_state.uploaded_df.columns
                or GENERATED_ANSWER_COLUMN in st.session_state.uploaded_df.columns
            )
        )
        if st.button(t("has_answer_btn"), width="stretch", disabled=not has_answer):
            if not st.session_state.api_key.strip():
                st.error(t("err_fill_api_key"))
            elif not st.session_state.scenario.strip() or not st.session_state.north_star.strip():
                st.error(t("err_fill_scenario"))
            else:
                with st.spinner("æ­£åœ¨ç”Ÿæˆè¯„æµ‹æ–¹æ¡ˆâ€¦"):
                    try:
                        prompt = generate_evaluator_prompt_in_app(
                            st.session_state.scenario,
                            st.session_state.north_star,
                            st.session_state.api_key,
                        )
                        st.session_state.generated_prompt = prompt
                        st.session_state.evaluation_prompt = prompt
                        st.session_state.phase = "PROMPT_EDIT"
                        st.success(t("success_eval_generated"))
                        st.rerun()
                    except Exception as e:
                        st.error(f"ç”Ÿæˆè¯„æµ‹æ–¹æ¡ˆå¤±è´¥ï¼ˆè¯·æ£€æŸ¥ API Key ä¸Žç½‘ç»œï¼‰ï¼š{e}")  # keep CN for technical msg


# ==================== Phase 2: ä¸šåŠ¡ Prompt ç¡®è®¤ï¼ˆå¯ç¼–è¾‘ï¼‰ ====================
def render_phase_generation_prompt_edit():
    st.subheader(t("phase2_title"))
    st.divider()
    st.caption(t("phase2_caption"))

    generation_prompt = st.text_area(
        t("business_prompt_label"),
        value=st.session_state.generation_prompt,
        height=280,
        help=t("business_prompt_help"),
    )
    st.session_state.generation_prompt = generation_prompt

    st.divider()
    if st.button(t("next_generate_eval"), type="primary", width="content"):
        if not (st.session_state.generation_prompt or "").strip():
            st.error(t("err_fill_business_prompt"))
            return
        with st.spinner("æ­£åœ¨æ ¹æ®åœºæ™¯ä¸ŽåŒ—æžæ˜ŸæŒ‡æ ‡ç”Ÿæˆè¯„ä¼° Promptâ€¦"):
            try:
                prompt = generate_evaluator_prompt_in_app(
                    st.session_state.scenario,
                    st.session_state.north_star,
                    st.session_state.api_key,
                )
                st.session_state.generated_prompt = prompt
                st.session_state.evaluation_prompt = prompt
                st.session_state.phase = "PROMPT_EDIT"
                st.success(t("success_gen_prompt"))
                st.rerun()
            except Exception as e:
                st.error(f"ç”Ÿæˆè¯„ä¼° Prompt å¤±è´¥ï¼ˆè¯·æ£€æŸ¥ API Key ä¸Žç½‘ç»œï¼‰ï¼š{e}")

    if st.button(t("back_config"), width="content"):
        st.session_state.phase = "CONFIG"
        st.rerun()


# ==================== Phase 4: ç”Ÿæˆå›žç­” ====================
def render_phase_generating():
    st.subheader(t("phase4_title"))
    st.divider()

    df = st.session_state.uploaded_df
    n = len(df) if df is not None else 0
    api_key = st.session_state.api_key
    model = st.session_state.model
    generation_prompt = (st.session_state.generation_prompt or "").strip()

    if not api_key or df is None or n == 0:
        st.error(t("err_config_incomplete"))
        if st.button(t("return_eval_prompt")):
            st.session_state.phase = "PROMPT_EDIT"
            st.rerun()
        return

    if GENERATED_ANSWER_COLUMN not in df.columns:
        df[GENERATED_ANSWER_COLUMN] = None

    # æœ‰é¢˜ç›®çš„è¡Œï¼šè‹¥å·²æœ‰ generated_answer æˆ– expected_answer ä»»ä¸€éžç©ºï¼Œè§†ä¸ºã€Œå·²æœ‰å›žç­”ã€
    def _has_answer(row):
        gen = str(row.get(GENERATED_ANSWER_COLUMN) or "").strip()
        exp = str(row.get(OPTIONAL_ANSWER_COLUMN) or "").strip() if OPTIONAL_ANSWER_COLUMN in df.columns else ""
        return bool(gen or exp)

    has_question = df["question"].notna() & (df["question"].astype(str).str.strip() != "")
    has_answer = df.apply(_has_answer, axis=1)
    rows_with_question = has_question.sum()
    rows_need_generation = (has_question & ~has_answer).sum()

    # è‹¥éœ€è¦ç”Ÿæˆä½†æœªé…ç½®ä¸šåŠ¡ Promptï¼Œæ‰æŠ¥é”™ï¼ˆè‡ªå·±ä¸Šä¼ å›žç­”æ—¶æ— éœ€ä¸šåŠ¡ Promptï¼‰
    if rows_need_generation > 0 and not generation_prompt:
        st.error(t("err_config_incomplete_no_prompt"))
        if st.button(t("return_eval_prompt")):
            st.session_state.phase = "PROMPT_EDIT"
            st.rerun()
        return

    # è‹¥æ‰€æœ‰æœ‰é¢˜ç›®çš„è¡Œå·²æœ‰å›žç­”ï¼ˆè‡ªå·±ä¸Šä¼ çš„ expected_answer æˆ–å·²æœ‰ generated_answerï¼‰ï¼Œç›´æŽ¥è¿›å…¥ã€Œä¸‹ä¸€æ­¥ã€
    if rows_with_question > 0 and rows_need_generation == 0:
        st.caption(t("caption_has_answers"))
        st.session_state.uploaded_df = df
        _render_generation_result_and_next(df, api_key)
        return

    progress_bar = st.progress(0.0, text=t("progress_preparing"))
    status = st.status(t("status_generating"), expanded=True)

    with status:
        for i, (idx, row) in enumerate(df.iterrows()):
            progress_bar.progress((i + 1) / n, text=f"{i+1}/{n}")
            q = str(row.get("question", "") or "").strip()
            st.write(f"[{i+1}/{n}] {q[:60]}â€¦" if len(q) > 60 else f"[{i+1}/{n}] {q}")
            if not q:
                df.at[idx, GENERATED_ANSWER_COLUMN] = ""
                st.write("  â­ " + t("skip_empty_question"))
                continue
            answer, err = run_single_generation(q, generation_prompt, api_key, model)
            if err:
                df.at[idx, GENERATED_ANSWER_COLUMN] = ""
                st.write(f"  âŒ {err}")
            else:
                df.at[idx, GENERATED_ANSWER_COLUMN] = answer or ""
                st.write("  âœ… " + t("gen_ok"))

    progress_bar.progress(1.0, text="ç”Ÿæˆå®Œæˆ")
    status.update(label="ç”Ÿæˆå®Œæˆ", state="complete")
    st.session_state.uploaded_df = df

    st.divider()
    _render_generation_result_and_next(df, api_key)


def _render_generation_result_and_next(df: pd.DataFrame, api_key: str):
    """å±•ç¤ºç”Ÿæˆç»“æžœï¼ˆåªè¯»ï¼‰ä¸Žã€Œä¸‹ä¸€æ­¥ï¼šå¼€å§‹è¯„æµ‹ã€æŒ‰é’®ï¼ˆè¯„ä¼° Prompt å·²åœ¨å‰é¢æ­¥éª¤ç”Ÿæˆå¹¶ç¡®è®¤ï¼‰ã€‚"""
    st.subheader(t("gen_result_title"))
    st.caption(t("gen_result_caption"))
    # ç”¨äºŽè¯„æµ‹çš„å›žç­”åˆ—ï¼šä¼˜å…ˆ generated_answerï¼Œå¦åˆ™ expected_answer
    answer_series = df.apply(
        lambda r: str(r.get(GENERATED_ANSWER_COLUMN) or r.get(OPTIONAL_ANSWER_COLUMN) or ""),
        axis=1,
    )
    display_df = df[["question"]].copy()
    display_df.columns = [t("col_question")]
    display_df[t("col_answer")] = answer_series
    st.dataframe(display_df, width="stretch", hide_index=True)
    st.divider()
    if st.button(t("next_start_eval"), type="primary", width="content"):
        st.session_state.phase = "EVALUATING"
        st.rerun()


# ==================== Phase 3: è¯„ä¼° Prompt ç¡®è®¤ï¼ˆå¯ç¼–è¾‘ï¼‰ ====================
def render_phase_prompt_edit():
    st.subheader(t("phase3_title"))
    st.divider()
    st.caption(t("phase3_caption"))

    evaluation_prompt = st.text_area(
        t("eval_prompt_label"),
        value=st.session_state.evaluation_prompt,
        height=320,
        help=t("eval_prompt_help"),
    )
    st.session_state.evaluation_prompt = evaluation_prompt

    if "{original_text}" not in evaluation_prompt or "{model_output}" not in evaluation_prompt:
        st.warning(t("placeholder_warning"))

    st.divider()
    if st.button(t("confirm_start"), type="primary", width="content"):
        if not st.session_state.evaluation_prompt.strip():
            st.error(t("err_fill_eval_prompt"))
            return
        st.session_state.phase = "GENERATING"
        st.rerun()

    if st.button(t("back_business_prompt"), width="content"):
        st.session_state.phase = "GENERATION_PROMPT_EDIT"
        st.rerun()


# ==================== Phase 5: æ‰§è¡Œè¯„æµ‹ ====================
def render_phase_evaluating():
    st.subheader(t("phase5_title"))
    st.divider()

    df = st.session_state.uploaded_df
    n = len(df) if df is not None else 0
    api_key = st.session_state.api_key
    model = st.session_state.model
    evaluation_prompt = st.session_state.evaluation_prompt

    if not api_key:
        st.error(t("err_api_key"))
        st.session_state.phase = "PROMPT_EDIT"
        return
    if df is None or n == 0:
        st.error(t("err_no_data"))
        st.session_state.phase = "CONFIG"
        return

    progress_bar = st.progress(0.0, text=t("progress_preparing"))
    status = st.status(t("status_evaluating"), expanded=True)

    eval_columns = [
        "eval_priority", "factuality_score", "north_star_score", "completeness_score",
        "weighted_total_score", "decision", "reason", "reasoning", "pass",
    ]
    for col in eval_columns:
        if col not in df.columns:
            df[col] = None

    start_time = time.time()
    with status:
        for i, (idx, row) in enumerate(df.iterrows()):
            progress_bar.progress((i + 1) / n, text=f"æ­£åœ¨è¯„æµ‹ç¬¬ {i+1}/{n} æ¡â€¦")
            st.write(f"[{i+1}/{n}] é¢˜ç›®: {str(row.get('question', ''))[:50]}â€¦")

            result, err = run_single_evaluation(row, evaluation_prompt, api_key, model)
            if err:
                df.at[idx, "decision"] = "ERROR"
                df.at[idx, "reason"] = f"error: {err}"
                st.write(f"  âŒ {err}")
            else:
                df.at[idx, "eval_priority"] = result.get("priority")
                df.at[idx, "factuality_score"] = result.get("factuality_score")
                df.at[idx, "north_star_score"] = result.get("north_star_score")
                df.at[idx, "completeness_score"] = result.get("completeness_score")
                df.at[idx, "weighted_total_score"] = result.get("weighted_total_score")
                df.at[idx, "decision"] = result.get("decision")
                df.at[idx, "reason"] = result.get("reason")
                df.at[idx, "reasoning"] = result.get("reasoning")
                df.at[idx, "pass"] = result.get("pass")
                st.write(f"  âœ… {t('eval_ok')}: {result.get('weighted_total_score', 0):.1f} | {result.get('decision', '')}")

    elapsed = time.time() - start_time
    progress_bar.progress(1.0, text="è¯„æµ‹å®Œæˆ")
    status.update(label="è¯„æµ‹å®Œæˆ", state="complete")

    st.session_state.results_df = df
    st.session_state.eval_elapsed = elapsed
    st.session_state.phase = "RESULT"
    st.success(t("success_eval_done", n=n, elapsed=elapsed))
    st.rerun()


# ==================== Phase 6: ç»“æžœå±•ç¤º ====================
def render_phase_result():
    st.subheader(t("phase6_title"))
    st.divider()

    df = st.session_state.results_df
    if df is None:
        st.warning(t("no_results"))
        return

    score_numeric = pd.to_numeric(df["weighted_total_score"], errors="coerce") if "weighted_total_score" in df.columns else pd.Series(dtype=float)
    valid = df[score_numeric.notna()]
    n_valid = len(valid)
    n_total = len(df)

    st.caption(t("core_metrics"))
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        avg_score = score_numeric.mean() if score_numeric.notna().any() else 0
        st.metric(t("avg_score"), f"{avg_score:.1f}" if score_numeric.notna().any() else "â€”")
    with col2:
        pass_count = valid["pass"].sum() if "pass" in valid.columns else (valid["decision"] == "PUBLISH").sum()
        pass_rate = (pass_count / n_valid * 100) if n_valid else 0
        st.metric(t("pass_rate"), f"{pass_rate:.1f}%" if n_valid else "â€”")
    with col3:
        err_count = (df["decision"] == "ERROR").sum()
        st.metric(t("error_count"), int(err_count))
    with col4:
        st.metric(t("total_count"), n_total)
    with col5:
        elapsed = st.session_state.get("eval_elapsed")
        st.metric(t("elapsed"), f"{elapsed:.1f} s" if elapsed is not None else "â€”")

    st.divider()

    if n_valid > 0 and "weighted_total_score" in df.columns:
        st.caption(t("score_dist"))
        score_series = pd.to_numeric(valid["weighted_total_score"], errors="coerce").dropna()
        if len(score_series) > 0:
            score_counts = score_series.round(0).value_counts().sort_index()
            fig = px.bar(
                x=score_counts.index.astype(int),
                y=score_counts.values,
                labels={"x": t("score_x"), "y": t("score_y")},
                title=t("score_dist_title"),
            )
            fig.update_layout(showlegend=False, margin=dict(t=40))
            st.plotly_chart(fig, width="stretch")
            if len(score_counts) <= 2 and score_counts.sum() > 5:
                with st.expander("ðŸ’¡ " + t("expander_diff_title"), expanded=False):
                    st.caption(t("expander_diff_caption"))
        else:
            st.caption(t("no_valid_scores"))
    st.divider()

    st.caption(t("prompts_section"))
    with st.expander(t("expand_business_prompt"), expanded=False):
        gen_prompt = st.session_state.get("generation_prompt") or ""
        st.text_area(t("expand_business_prompt"), value=gen_prompt, height=200, disabled=True, label_visibility="collapsed")
    with st.expander(t("expand_eval_prompt"), expanded=False):
        eval_prompt = st.session_state.get("evaluation_prompt") or ""
        st.text_area(t("expand_eval_prompt"), value=eval_prompt, height=280, disabled=True, label_visibility="collapsed")
    st.divider()

    st.caption(t("full_results_caption"))
    st.caption(t("reject_note"))
    display_cols = [
        "question", GENERATED_ANSWER_COLUMN, OPTIONAL_ANSWER_COLUMN,
        "factuality_score", "north_star_score", "completeness_score",
        "weighted_total_score", "decision", "reason", "reasoning",
    ]
    display_cols = [c for c in display_cols if c in df.columns]
    col_config = {}
    for col, label in [
        ("factuality_score", t("col_factuality")),
        ("north_star_score", t("col_north_star")),
        ("completeness_score", t("col_completeness")),
        ("weighted_total_score", t("col_weighted_total")),
    ]:
        if col in display_cols:
            col_config[col] = st.column_config.NumberColumn(label, format="%.1f")
    st.dataframe(
        df[display_cols] if display_cols else df,
        width="stretch",
        hide_index=True,
        column_config=col_config if col_config else None,
    )

    st.divider()
    buf = io.BytesIO()
    df.to_csv(buf, index=False, encoding="utf-8-sig")
    st.download_button(
        label=t("download_csv"),
        data=buf.getvalue(),
        file_name="eval_results.csv",
        mime="text/csv",
    )


# ==================== Main ====================
def main():
    init_session_state()
    render_sidebar()

    st.title("ðŸ“Š " + t("app_title"))
    st.caption(t("breadcrumb"))
    st.divider()

    phase = st.session_state.phase
    if phase == "CONFIG":
        render_phase_config()
    elif phase == "GENERATION_PROMPT_EDIT":
        render_phase_generation_prompt_edit()
    elif phase == "GENERATING":
        render_phase_generating()
    elif phase == "PROMPT_EDIT":
        render_phase_prompt_edit()
    elif phase == "EVALUATING":
        render_phase_evaluating()
    elif phase == "RESULT":
        render_phase_result()
    else:
        st.session_state.phase = "CONFIG"
        st.rerun()


if __name__ == "__main__":
    main()
